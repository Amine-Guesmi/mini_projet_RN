# mini_projet_RN

L’architecture se compose de : 
3 Feature Inputs 

Block 1 :
10 convolution layers
10 RELU (rectified Linear Unit) comme fonction d’activation 
10 convolution layers
10 RELU (rectified Linear Unit) comme fonction d’activation 
Max Pool pour régulariser le modèle (éliminer le sur-apprentissage) 

Dropout Layer : Régulariser le modèle en éliminant aléatoirement quelques convolution layer avec pourcentage de 0.2

Block 2 :
10 convolution layers
10 RELU (rectified Linear Unit) comme fonction d’activation 
10 convolution layers
10 RELU (rectified Linear Unit) comme fonction d’activation 
Max Pool pour régulariser le modèle (éliminer le sur-apprentissage) 

